# 爬虫软件需求文档

## 可视化需求
用户可以在可视化的界面输入爬取的网址。（前端输入，后端以json格式接收，并保存为config.json）√
用户可以输入自己想要爬取的网址字段对应的X path表达式。（同上）√
由于爬虫软件是针对爬取政策的，需要有对应翻页的X path表达式输入处理。（同上）√
美观且简洁的页面。页面一看到就容易理解该如何操作。（前端设计部分，目前设计是：初始有输入框，爬虫按钮，点击爬虫按钮后，可以选择存储的格式以及存储的位置。点击爬虫后，应该有个文本框，可以显示爬取信息）√

## 快速上手使用需求
一些前置依赖要求，比如爬虫的Splash服务，如果能使用一键脚本安装，则对用户给出对应快速部署脚本。（难搞，目前是用docker来部署，用户还需要下载docker后搞一个splash）
如果做不到，则对用户给出程序的详细使用说明。
如果可以，还应该给出用户在遇到因电脑配置不同等问题时的解决方案。

## 存储需求
用户可以选择将政策存储到CSV文件，xlsx文件或者PostgreSQL数据库中。（数据库暂时不清楚怎么做）
用户可以定义文件存储路径。

## 自定义配置需求
用户需要能自定义爬取地址。√
用户需要能自定义爬取时间，当到达指定时间后，应该先将目前爬取到的全部政策存储起来。（需要修改爬虫代码，可以使用scrapy的CLOSESPIDER_TIMEOUT来配置）
用户可以自定义存储数据库的地址。（暂时没有存入数据库的打算）
用户可以爬取网址上自己需要的字段。（将更多的字段变为content）
支持自定义请求间隔（如 1-5 秒 / 次），避免因请求过快触发网站反爬机制。（可以通过在 SplashRequest 的 args 参数中设置 wait 字段，来控制 Splash 的渲染等待时间。在 config.json 里加一个 wait 字段，然后在代码中读取并传递给 args['wait']。）
支持按核心字段（如政策 URL、标题）自动去重，用户可选择去重规则。

## 监控需求
用户需要看到自己爬取到的政策网址，政策文本等信息。√
用户需要在爬取对应网址的最后看到汇总信息（比如，总共爬取到多少条政策）。
用户需要看到爬取对应政策，但是没有信息或者网址链接超时等特殊情况时看到报错信息。
用户需要看到全部政策信息在写入数据库或者写入文件后，看到提示写入成功信息。

## 异常处理需求
针对网络波动、服务器超时（如 5xx 错误）、连接中断等情况，自动重试爬取。重试一定次数后放弃。
自动记录爬取全过程日志。（好像没有必要）