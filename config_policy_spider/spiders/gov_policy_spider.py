import json
import logging

import scrapy
from scrapy_splash import SplashRequest


class GovPolicySpider(scrapy.Spider):
    name = "gov_policy"
    custom_settings = {
        # Splash æœåŠ¡ URLï¼Œè¯·ç¡®ä¿ä½ çš„ Splash æœåŠ¡æ­£åœ¨è¿è¡Œ
        'SPLASH_URL': 'http://localhost:8050',
        # å¿…è¦çš„ Splash ä¸­é—´ä»¶é…ç½®
        'DOWNLOADER_MIDDLEWARES': {
            'scrapy_splash.SplashCookiesMiddleware': 723,
            'scrapy_splash.SplashMiddleware': 725,
            'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
        },
        'SPIDER_MIDDLEWARES': {
            'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,
        },
        'DUPEFILTER_CLASS': 'scrapy_splash.SplashAwareDupeFilter',
        'HTTPCACHE_STORAGE': 'scrapy_splash.SplashAwareFSCacheStorage',
        # è¾“å‡º JSON æ–‡ä»¶é…ç½®
        'FEED_FORMAT': 'json',
        'FEED_URI': 'policies.json',
        'FEED_EXPORT_ENCODING': 'utf-8',
        # æ—¥å¿—çº§åˆ« (DEBUG ä¼šè¾“å‡ºè¯¦ç»†ä¿¡æ¯ï¼Œä¾¿äºè°ƒè¯•)
        'LOG_LEVEL': 'DEBUG',
        # è®¾ç½®é»˜è®¤è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
        'DEFAULT_REQUEST_HEADERS': {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en;q=0.6',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36',
        }
    }

    def start_requests(self):
        # è¯»å–é…ç½®
        with open('config.json', encoding='utf-8') as f:
            cfg = json.load(f)[0] # å‡è®¾ config.json å§‹ç»ˆæ˜¯ä¸€ä¸ªåŒ…å«å•ä¸ªé…ç½®å¯¹è±¡çš„åˆ—è¡¨
        self.site_name = cfg['name']
        self.selectors = cfg['selectors']
        start_url = cfg['url']

        self.logger.debug(f"ğŸš€ å¼€å§‹æŠ“å–: {self.site_name} â†’ {start_url}")
        yield SplashRequest(
            url=start_url,
            callback=self.parse_list,
            args={
                'wait': 3,  # **å¢åŠ ç­‰å¾…æ—¶é—´**ï¼Œç¡®ä¿é¡µé¢å†…å®¹åŠ è½½å®Œæˆ
                'render_all': 1 # å°è¯•æ¸²æŸ“æ‰€æœ‰èµ„æºï¼Œç¡®ä¿æ‰€æœ‰JSå†…å®¹åŠ è½½
            },
            dont_filter=True
        )

    def parse_list(self, response):
        self.logger.debug(f"ğŸ“„ è§£æåˆ—è¡¨é¡µ: {response.url}")

        # **è°ƒè¯•æ­¥éª¤ï¼šå°† Splash æ¸²æŸ“çš„ HTML å†…å®¹ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶**
        # è¿è¡Œçˆ¬è™«åï¼Œæ‰“å¼€ 'debug_list_page.html' æ–‡ä»¶ï¼Œåœ¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·ä¸­éªŒè¯ä½ çš„ XPath
        with open('debug_list_page.html', 'w', encoding='utf-8') as f:
            f.write(response.text)
        self.logger.debug("å·²å°†åˆ—è¡¨é¡µå“åº”å†…å®¹ä¿å­˜åˆ° 'debug_list_page.html' è¿›è¡Œè°ƒè¯•ã€‚")


        # è·å–æ ‡é¢˜ä¸é“¾æ¥
        # é’ˆå¯¹å¹¿ä¸œçœäººæ°‘æ”¿åºœç½‘ç«™çš„ç‰¹ç‚¹ï¼Œå¯ä»¥å°è¯•æ›´å…·ä½“çš„ XPath
        # ä¾‹å¦‚ï¼Œæ£€æŸ¥ `zwgk_list` div ä¸‹æ˜¯å¦ç›´æ¥æ˜¯ `ul/li/a` ç»“æ„
        titles = response.xpath(self.selectors['title']).getall()
        links  = response.xpath(self.selectors['link']).getall()

        self.logger.debug(f"ğŸ” åˆ—è¡¨é¡µå…±æ‰¾åˆ° {len(titles)} æ¡æ ‡é¢˜ï¼Œ{len(links)} æ¡é“¾æ¥")

        # æ£€æŸ¥æ˜¯å¦è·å–åˆ°å†…å®¹
        if not titles or not links:
            self.logger.warning(f"âš ï¸ åœ¨ {response.url} æœªæ‰¾åˆ°æ ‡é¢˜æˆ–é“¾æ¥ï¼Œè¯·æ£€æŸ¥ XPath é€‰æ‹©å™¨æˆ–é¡µé¢åŠ è½½é—®é¢˜ã€‚")

        for title, href in zip(titles, links):
            title = title.strip()
            detail_url = response.urljoin(href)
            self.logger.debug(f"â¡ï¸ å‡†å¤‡æŠ“å–è¯¦æƒ…: {title} â†’ {detail_url}")
            yield SplashRequest(
                url=detail_url,
                callback=self.parse_detail,
                meta={'title': title},
                args={'wait': 1}, # è¯¦æƒ…é¡µé€šå¸¸ä¹Ÿéœ€è¦ç­‰å¾…
            )

        # ç¿»é¡µå¤„ç†
        # æ£€æŸ¥ 'ä¸‹ä¸€é¡µ' é“¾æ¥çš„ XPath æ˜¯å¦å‡†ç¡®
        # æœ‰æ—¶ 'ä¸‹ä¸€é¡µ' æ–‡æœ¬å¯èƒ½è¢«åŒ…è£¹åœ¨å…¶ä»–æ ‡ç­¾ä¸­ï¼Œæˆ–è€…æœ‰é¢å¤–çš„ç©ºæ ¼
        next_href = response.xpath(self.selectors['next_page']).get()
        if next_href:
            next_url = response.urljoin(next_href)
            self.logger.debug(f"ğŸ”œ è·Ÿè¿›ä¸‹ä¸€é¡µ: {next_url}")
            yield SplashRequest(
                url=next_url,
                callback=self.parse_list,
                args={'wait': 3}, # ç¿»é¡µåŒæ ·éœ€è¦ç­‰å¾…
            )
        else:
            self.logger.debug("ğŸš« æ²¡æœ‰æ‰¾åˆ°ä¸‹ä¸€é¡µï¼Œåˆ—è¡¨è§£æç»“æŸã€‚")

    def parse_detail(self, response):
        title = response.meta['title']
        self.logger.debug(f"ğŸ“° è§£æè¯¦æƒ…é¡µ: {title} â†’ {response.url}")

        # **è°ƒè¯•æ­¥éª¤ï¼šå°†è¯¦æƒ…é¡µå“åº”å†…å®¹ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶**
        # è¿è¡Œçˆ¬è™«åï¼Œæ‰“å¼€ 'debug_detail_page.html' æ–‡ä»¶ï¼ŒéªŒè¯ä½ çš„å†…å®¹ XPath
        with open('debug_detail_page.html', 'w', encoding='utf-8') as f:
            f.write(response.text)
        self.logger.debug(f"å·²å°†è¯¦æƒ…é¡µå“åº”å†…å®¹ä¿å­˜åˆ° 'debug_detail_page.html' è¿›è¡Œè°ƒè¯•ã€‚")

        # æå–æ­£æ–‡æ®µè½
        paras = response.xpath(self.selectors['content']).getall()
        content = "\n".join(p.strip() for p in paras if p.strip())
        self.logger.info(f"âœ… å·²æŠ“å–ã€Š{title}ã€‹ï¼Œæ­£æ–‡å…± {len(content)} å­—ç¬¦")

        yield {
            'site':    self.site_name,
            'title':   title,
            'url':     response.url,
            'content': content,
        }